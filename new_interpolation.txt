Iterative Text Expander Component Blueprint (Developer Specification)
1. System Overview
A Python-based text expansion module that iteratively inserts relevant content between sentences using local documents. The system progresses through sentence pairs until no suitable insertions remain.

2. Component Architecture

Diagram
Code



















3. Core Components Specification

3.1 Document Selector

Input: Draft text + directory of .txt files

Process:

Compute TF-IDF between draft and each document

Fallback to BERT similarity if TF-IDF < 0.3

Return top 15 relevant documents (max 10x draft length)

Output: Filtered document list

3.2 Sentence Segmenter

Requirements:

Preserve technical terms (e.g., "Fig. 1A" not split)

Maintain list formatting (bullets/numbers)

Handle multilingual punctuation

Libraries: spaCy sentencizer (disable parser/tagger)

3.3 ANN Index

Structure:

python
{
    "sentence": str,
    "embedding": float32[384],
    "source_doc": str,
    "position": int
}
Technology: FAISS IVF-PQ index (nlist=100)

Operations: add(), search(queries, k=10)

3.4 Bridge Retriever

Query Logic:
query_embed = model.encode(f"{prev_sent} [BRIDGE] {next_sent}")

Features:

Transition word boost (+0.2 score for "however/furthermore")

Context window (previous 2 sentences considered)

3.5 Scoring Engine
Scoring Formula:
total_score = (0.5 * semantic_sim) + (0.3 * novelty) + (0.2 * flow)

Metrics:

semantic_sim: Cosine similarity to gap context

novelty: 1 - (max n-gram overlap with existing text)

flow: Transition word match + position proximity

Thresholds:

Min insertion score: 0.65

Novelty requirement: <40% 4-gram overlap

3.6 Iteration Controller

python
def expand_iteratively(draft: str, corpus_dir: str) -> str:
    sentences = segment(draft)
    relevant_docs = select_docs(draft, corpus_dir)
    index = build_index(relevant_docs)
    expanded = [sentences[0]]
    
    for i in range(1, len(sentences)):
        candidates = retrieve_bridges(
            prev=expanded[-1], 
            next=sentences[i],
            context=" ".join(expanded[-2:])
        )
        
        if candidates:
            best = score_candidates(candidates)
            if best['score'] > 0.65:
                expanded.append(best['text'])
        
        expanded.append(sentences[i])
    
    return correct_grammar(" ".join(expanded))
3.7 Termination Conditions

Condition	Action
No candidates found	Proceed to next pair
Best score < 0.65	Skip insertion
Novelty score < 0.4	Reject candidate
40% length increase reached	Halt further expansion
3 passes completed	Return current result
4. Interface Contracts

python
class TextExpander:
    def __init__(self, corpus_path: str, config: dict = DEFAULT_CONFIG):
        """Initialize with document corpus"""
        
    def expand(
        self, 
        text: str, 
        max_iterations: int = 3,
        expansion_limit: float = 0.4
    ) -> dict:
        """
        Returns:
        {
            "expanded_text": str,
            "insertions": [
                {"position": int, "sentence": str, "source": str},
                ...
            ],
            "metrics": {
                "original_length": int,
                "final_length": int,
                "iteration_count": int
            }
        }
        """
5. Performance Requirements

Input Limits:

Max draft length: 5,000 words

Max corpus size: 10,000 documents

Resource Constraints:

Memory: <1GB for 10K-document corpus

Processing: <15s for 500-word draft on i5 CPU

Output Quality:

Coherence: >0.90 BERTScore F1

Novelty: <30% n-gram overlap with source

6. Validation Test Cases

Test 1: Basic Insertion

Input Draft: "Photosynthesis requires light. Plants convert CO2 to oxygen."

Expected Insert: "Chlorophyll captures light energy for this process."

Test 2: Termination Logic

python
# Should terminate after 2 insertions
expander = TextExpander(corpus_path, {"max_iterations": 3})
result = expander.expand(text, expansion_limit=0.3)
assert result["metrics"]["iteration_count"] == 2
Test 3: Technical Term Handling

Input: "The CRISPR-Cas9 system enables gene editing. It uses guide RNA."

Verification: Ensure "CRISPR-Cas9" remains intact during segmentation

Test 4: Resource Boundary

python
# 10K docs should stay under 1GB RAM
with measure_memory():
    expander = TextExpander(large_corpus)
    assert peak_memory < 1024  # MB
7. Development Milestones

Week 1: Implement DocumentSelector + SentenceSegmenter

Week 2: Build ANN Index + BridgeRetriever

Week 3: Develop Scoring Engine + Iteration Controller

Week 4: Integrate Grammar Correction + Validation Suite

Week 5: Performance Optimization + Edge Case Handling

8. Dependencies

Library	Purpose	Version
sentence-transformers	Embedding generation	>=2.2.0
faiss-cpu	Efficient similarity search	>=1.7.3
spacy	Sentence segmentation	>=3.5.0
language-tool-python	Grammar correction	>=2.7.0
This blueprint provides complete implementation specifications while maintaining flexibility for developer innovation in scoring algorithms and optimization strategies. The iterative approach ensures natural expansion while preventing over-insertion through scientifically validated termination heuristics.
